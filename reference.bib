@inproceedings{dctcp,
author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A. and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari},
title = {Data center TCP (DCTCP)},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851192},
doi = {10.1145/1851182.1851192},
abstract = {Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry "background" flows build up queues at the switches, and thus impact the performance of latency sensitive "foreground" traffic.To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90\% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {63–74},
numpages = {12},
keywords = {ECN, TCP, data center network},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@inproceedings{dcqcn,
author = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming},
title = {Congestion Control for Large-Scale RDMA Deployments},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787484},
doi = {10.1145/2785956.2787484},
abstract = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (&lt; 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {523–536},
numpages = {14},
keywords = {ECN, PFC, RDMA, congestion control, datacenter transport},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{swift,
author = {Kumar, Gautam and Dukkipati, Nandita and Jang, Keon and Wassel, Hassan M. G. and Wu, Xian and Montazeri, Behnam and Wang, Yaogong and Springborn, Kevin and Alfeld, Christopher and Ryan, Michael and Wetherall, David and Vahdat, Amin},
title = {Swift: Delay is Simple and Effective for Congestion Control in the Datacenter},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3406591},
doi = {10.1145/3387514.3406591},
abstract = {We report on experiences with Swift congestion control in Google datacenters. Swift targets an end-to-end delay by using AIMD control, with pacing under extreme congestion. With accurate RTT measurement and care in reasoning about delay targets, we find this design is a foundation for excellent performance when network distances are well-known. Importantly, its simplicity helps us to meet operational challenges. Delay is easy to decompose into fabric and host components to separate concerns, and effortless to deploy and maintain as a congestion signal while the datacenter evolves. In large-scale testbed experiments, Swift delivers a tail latency of <50μs for short RPCs, with near-zero packet drops, while sustaining ~100Gbps throughput per server. This is a tail of <3x the minimal latency at a load close to 100\%. In production use in many different clusters, Swift achieves consistently low tail completion times for short RPCs, while providing high throughput for long RPCs. It has loss rates that are at least 10x lower than a DCTCP protocol, and handles O(10k) incasts that sharply degrade with DCTCP.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {514–528},
numpages = {15},
keywords = {Congestion Control, Datacenter Transport, Performance Isolation},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{timely,
author = {Mittal, Radhika and Lam, Vinh The and Dukkipati, Nandita and Blem, Emily and Wassel, Hassan and Ghobadi, Monia and Vahdat, Amin and Wang, Yaogong and Wetherall, David and Zats, David},
title = {TIMELY: RTT-based Congestion Control for the Datacenter},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787510},
doi = {10.1145/2785956.2787510},
abstract = {Datacenter transports aim to deliver low latency messaging together with high throughput. We show that simple packet delay, measured as round-trip times at hosts, is an effective congestion signal without the need for switch feedback. First, we show that advances in NIC hardware have made RTT measurement possible with microsecond accuracy, and that these RTTs are sufficient to estimate switch queueing. Then we describe how TIMELY can adjust transmission rates using RTT gradients to keep packet latency low while delivering high bandwidth. We implement our design in host software running over NICs with OS-bypass capabilities. We show using experiments with up to hundreds of machines on a Clos network topology that it provides excellent performance: turning on TIMELY for OS-bypass messaging over a fabric with PFC lowers 99 percentile tail latency by 9X while maintaining near line-rate throughput. Our system also outperforms DCTCP running in an optimized kernel, reducing tail latency by $13$X. To the best of our knowledge, TIMELY is the first delay-based congestion control protocol for use in the datacenter, and it achieves its results despite having an order of magnitude fewer RTT signals (due to NIC offload) than earlier delay-based schemes such as Vegas.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {537–550},
numpages = {14},
keywords = {rdma, os-bypass, delay-based congestion control, datacenter transport},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{hpcc,
author = {Li, Yuliang and Miao, Rui and Liu, Hongqiang Harry and Zhuang, Yan and Feng, Fei and Tang, Lingbo and Cao, Zheng and Zhang, Ming and Kelly, Frank and Alizadeh, Mohammad and Yu, Minlan},
title = {HPCC: high precision congestion control},
year = {2019},
isbn = {9781450359566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341302.3342085},
doi = {10.1145/3341302.3342085},
abstract = {Congestion control (CC) is the key to achieving ultra-low latency, high bandwidth and network stability in high-speed networks. From years of experience operating large-scale and high-speed RDMA networks, we find the existing high-speed CC schemes have inherent limitations for reaching these goals. In this paper, we present HPCC (High Precision Congestion Control), a new high-speed CC mechanism which achieves the three goals simultaneously. HPCC leverages in-network telemetry (INT) to obtain precise link load information and controls traffic precisely. By addressing challenges such as delayed INT information during congestion and overreac-tion to INT information, HPCC can quickly converge to utilize free bandwidth while avoiding congestion, and can maintain near-zero in-network queues for ultra-low latency. HPCC is also fair and easy to deploy in hardware. We implement HPCC with commodity programmable NICs and switches. In our evaluation, compared to DCQCN and TIMELY, HPCC shortens flow completion times by up to 95\%, causing little congestion even under large-scale incasts.},
booktitle = {Proceedings of the ACM Special Interest Group on Data Communication},
pages = {44–58},
numpages = {15},
keywords = {smart NIC, programmable switch, congestion control, RDMA},
location = {Beijing, China},
series = {SIGCOMM '19}
}

@inproceedings {poseidon,
author = {Weitao Wang and Masoud Moshref and Yuliang Li and Gautam Kumar and T. S. Eugene Ng and Neal Cardwell and Nandita Dukkipati},
title = {Poseidon: Efficient, Robust, and Practical Datacenter {CC} via Deployable {INT}},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {255--274},
url = {https://www.usenix.org/conference/nsdi23/presentation/wang-weitao},
publisher = {USENIX Association},
month = apr
}

@inproceedings{rocc,
author = {Taheri, Parvin and Menikkumbura, Danushka and Vanini, Erico and Fahmy, Sonia and Eugster, Patrick and Edsall, Tom},
title = {RoCC: robust congestion control for RDMA},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431316},
doi = {10.1145/3386367.3431316},
abstract = {In this paper, we present RoCC, a robust congestion control approach for datacenter networks based on RDMA. RoCC leverages switch queue size as an input to a PI controller, which computes the fair data rate of flows in the queue, signaling it to the flow sources. The PI parameters are self-tuning to guarantee stability, rapid convergence, and fair and near-optimal throughput in a wide range of congestion scenarios. Our simulation and DPDK implementation results show that RoCC can achieve up to 7\texttimes{} reduction in PFC frames generated under high average load levels, compared to DCQCN. At the same time, RoCC can achieve up to 8\texttimes{} lower tail latency, compared to DCQCN and HPCC. We also find that RoCC does not require PFC. The functional components of RoCC are implementable in P4-based and fixed-function switch ASICs.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {17–30},
numpages = {14},
keywords = {RDMA, congestion control, datacenter, network programmability},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

@inproceedings {acc,
author = {Yiran Zhang and Qingkai Meng and Chaolei Hu and Fengyuan Ren},
title = {Revisiting Congestion Control for Lossless Ethernet},
booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
year = {2024},
isbn = {978-1-939133-39-7},
address = {Santa Clara, CA},
pages = {131--148},
url = {https://www.usenix.org/conference/nsdi24/presentation/zhang-yiran},
publisher = {USENIX Association},
month = apr
}

@inproceedings{irn,
author = {Mittal, Radhika and Shpiner, Alexander and Panda, Aurojit and Zahavi, Eitan and Krishnamurthy, Arvind and Ratnasamy, Sylvia and Shenker, Scott},
title = {Revisiting network support for RDMA},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230557},
doi = {10.1145/3230543.3230557},
abstract = {The advent of RoCE (RDMA over Converged Ethernet) has led to a significant increase in the use of RDMA in datacenter networks. To achieve good performance, RoCE requires a lossless network which is in turn achieved by enabling Priority Flow Control (PFC) within the network. However, PFC brings with it a host of problems such as head-of-the-line blocking, congestion spreading, and occasional deadlocks. Rather than seek to fix these issues, we instead ask: is PFC fundamentally required to support RDMA over Ethernet?We show that the need for PFC is an artifact of current RoCE NIC designs rather than a fundamental requirement. We propose an improved RoCE NIC (IRN) design that makes a few simple changes to the RoCE NIC for better handling of packet losses. We show that IRN (without PFC) outperforms RoCE (with PFC) by 6-83\% for typical network scenarios. Thus not only does IRN eliminate the need for PFC, it improves performance in the process! We further show that the changes that IRN introduces can be implemented with modest overheads of about 3-10\% to NIC resources. Based on our results, we argue that research and industry should rethink the current trajectory of network support for RDMA.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {313–326},
numpages = {14},
keywords = {iWARP, datacenter transport, RoCE, RDMA, PFC},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@inproceedings{ecn-delay,
author = {Zhu, Yibo and Ghobadi, Monia and Misra, Vishal and Padhye, Jitendra},
title = {ECN or Delay: Lessons Learnt from Analysis of DCQCN and TIMELY},
year = {2016},
isbn = {9781450342926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2999572.2999593},
doi = {10.1145/2999572.2999593},
abstract = {Data center networks, and especially drop-free RoCEv2 networks require efficient congestion control protocols. DCQCN (ECN-based) and TIMELY (delay-based) are two recent proposals for this purpose. In this paper, we analyze DCQCN and TIMELY using fluid models and simulations, for stability, convergence, fairness and flow completion time. We uncover several surprising behaviors of these protocols. For example, we show that DCQCN exhibits non-monotonic stability behavior, and that TIMELY can converge to stable regime with arbitrary unfairness. We propose simple fixes and tuning for ensuring that both protocols converge to and are stable at the fair share point. Finally, using lessons learnt from the analysis, we address the broader question: are there fundamental reasons to prefer either ECN or delay for end-to-end congestion control in data center networks? We argue that ECN is a better congestion signal, due to the way modern switches mark packets, and due to a fundamental limitation of end-to-end delay-based protocols, that we derive.},
booktitle = {Proceedings of the 12th International on Conference on Emerging Networking EXperiments and Technologies},
pages = {313–327},
numpages = {15},
keywords = {congestion control, data center transport, delay-based, ecn, rdma},
location = {Irvine, California, USA},
series = {CoNEXT '16}
}

@misc{pfc-qbb,
  author = {{IEEE}},
  title = {{802.1Qbb} - Priority-Based Flow Control},
  year = {2011}
}

@misc{qcn,
  author = {{IEEE}},
  title = {{802.1Qau} - Congestion Notification},
  year = {2010}
}

@misc{int,
  author = {P4.org},
  title = {In-band Network Telemetry (INT) Dataplane Specification},
  year = {2020}
}

@misc{ecn,
    series =    {Request for Comments},
    number =    3168,
    howpublished =  {RFC 3168},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC3168},
    url =       {https://www.rfc-editor.org/info/rfc3168},
    author =    {Sally Floyd and Dr. K. K. Ramakrishnan and David L. Black},
    title =     {{The Addition of Explicit Congestion Notification (ECN) to IP}},
    pagetotal = 63,
    year =      2001,
    month =     sep,
    abstract =  {This memo specifies the incorporation of ECN (Explicit Congestion Notification) to TCP and IP, including ECN's use of two bits in the IP header. {[}STANDARDS-TRACK{]}},
}

@misc{red,
    series =    {Request for Comments},
    number =    2309,
    howpublished =  {RFC 2309},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC2309},
    url =       {https://www.rfc-editor.org/info/rfc2309},
    author =    {Lixia Zhang and Dr. Craig Partridge and Scott Shenker and John T. Wroclawski and Dr. K. K. Ramakrishnan and Larry Peterson and Dr. David D. Clark and Greg Minshall and Jon Crowcroft and Robert T. Braden and Dr. Steve E. Deering and Sally Floyd and Dr. Bruce S. Davie and Van Jacobson and Dr. Deborah Estrin},
    title =     {{Recommendations on Queue Management and Congestion Avoidance in the Internet}},
    pagetotal = 17,
    year =      1998,
    month =     apr,
    day =       1,
    abstract =  {This memo presents two recommendations to the Internet community concerning measures to improve and preserve Internet performance. It presents a strong recommendation for testing, standardization, and widespread deployment of active queue management in routers, to improve the performance of today's Internet. It also urges a concerted effort of research, measurement, and ultimate deployment of router mechanisms to protect the Internet from flows that are not sufficiently responsive to congestion notification. This memo provides information for the Internet community. It does not specify an Internet standard of any kind.},
}

@inproceedings{tcd,
author = {Zhang, Yiran and Liu, Yifan and Meng, Qingkai and Ren, Fengyuan},
title = {Congestion detection in lossless networks},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472899},
doi = {10.1145/3452296.3472899},
abstract = {Congestion detection is the cornerstone of end-to-end congestion control. Through in-depth observations and understandings, we reveal that existing congestion detection mechanisms in mainstream lossless networks (i.e., Converged Enhanced Ethernet and InfiniBand) are improper, due to failing to cognize the interaction between hop-by-hop flow controls and congestion detection behaviors in switches. We define ternary states of switch ports and present Ternary Congestion Detection (TCD) for mainstream lossless networks. Testbed and extensive simulations demonstrate that TCD can detect congestion ports accurately and identify flows contributing to congestion as well as flows only affected by hop-by-hop flow controls. Meanwhile, we shed light on how to incorporate TCD with rate control. Case studies show that existing congestion control algorithms can achieve 3.3x and 2.0x better median and 99th-percentile FCT slowdown by combining with TCD.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {370–383},
numpages = {14},
keywords = {congestion detection, flow control, lossless networks},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}